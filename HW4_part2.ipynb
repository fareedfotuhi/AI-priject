{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\t\t\n",
    "<p></p>\n",
    "<p></p>\n",
    "<font size=5>\n",
    "In the Name of God\n",
    "<font/>\n",
    " <p></p>\n",
    " <br/>\n",
    "\t\t<div align=center>\n",
    "\t\t    <size=3.5>\n",
    "\t\t\t    <br />\n",
    "Practical Assignment 4- part 2\n",
    "              <font  size=4>\n",
    "            \t<br/>\n",
    "              N-grams\n",
    "\t\t\t</font>\n",
    "    <br/>\n",
    "    <br/>\n",
    " </div>\n",
    "<hr/>\n",
    "Artifical Intelligence - Dr. GholamReza GhassemSani\n",
    "</font>\n",
    "  <p></p>\n",
    " <br/>\n",
    "Sharif University of Technology\n",
    "<p></p>\n",
    "Spring 2022\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<font size=4 color=red>\n",
    " </font>\n",
    "                <br/>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Assignment Details\n",
    "**Due date: 21/4/1401 (at 11:59pm)**<br>\n",
    "You are free to collaborate but solutions must be written up individually.\n",
    "Collaborators **must** be acknowledged.<br>\n",
    "Submissions with more than 100 hours delay will not be graded.<br>Submissions with less than\n",
    "50 hours delay will be penalized by the following rule:<br>\n",
    "**Penalized mark = M * (100 – D) / 100** <br>\n",
    "Where M = the mark achieved from your solution and D is number of hours passed the\n",
    "deadline. Submissions within 50 < X ≤ 100 hours delay will be penalized by P.M. = M * 0.5.<br>\n",
    "Submit your answers on quera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "student_number = 98110073<br>\n",
    "Name = Farid<br>\n",
    "Last_Name = Fotoohi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this assignment you will be implementing N-gram language models. N-gram language models assume that each word $wi$\n",
    "depends only on the N−1 preceding words. you will be using these language models for:\n",
    "*   calculating the probability of sentences\n",
    "*   predicting next word of a query\n",
    "\n",
    "###  Dataset\n",
    "Download dataset from [here](https://drive.google.com/drive/folders/1OOjVMG61mrBmGDh2KgPrO0HqrMPj-M0j?usp=sharing).  \n",
    "The dataset you will be working with for this assignment are available as `train.pickle` and `test.pickle` files. Each file includes a list of lists and each list contains tokens of a sentences/sentences.  \n",
    " Run the codes below to know more about the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/train.pickle\", 'rb') as handle:\n",
    "    train_data = pickle.load(handle)\n",
    "\n",
    "with open(\"data/test.pickle\", 'rb') as handle:\n",
    "    test_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we have a sample tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chillin',\n",
       " 'on',\n",
       " 'it',\n",
       " 'waiting',\n",
       " 'on',\n",
       " 'the',\n",
       " 'fam',\n",
       " 'to',\n",
       " 'get',\n",
       " 'in',\n",
       " 'wats',\n",
       " 'good',\n",
       " 'wit',\n",
       " 'u']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task 1 (Preprocessing)\n",
    "At first explain what other preprocessing does the data need and and why then implement your suggested preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explain your answers here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "\n",
    "def concat_texts(text):\n",
    "    st = \"\"\n",
    "    for i in text:\n",
    "        if i != ' ':\n",
    "            st += i +\" \"\n",
    "    return st\n",
    "    \n",
    "def remove_punctuation(text):\n",
    "    if(type(text)==float):\n",
    "        return text\n",
    "    ans=\"\"  \n",
    "    for i in text:     \n",
    "        if i not in string.punctuation:\n",
    "            ans+= i  \n",
    "    return ans.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [concat_texts(item) for item in train_data]\n",
    "train_data = [remove_punctuation(item) for item in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chillin on it waiting on the fam to get in wats good wit u'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (Counting N-grams)\n",
    "Fill out the following code block to count the unigrams, bigrams and trigrams in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_N_grams(text, ngram):\n",
    "    words=[word for word in text.split(\" \")]\n",
    "    temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "    ans=[' '.join(ngram) for ngram in temp]\n",
    "    return ans\n",
    "\n",
    "def LanguageModel(data):\n",
    "    ### you need to use your preprocessed data and count the unigram,bigram and trigrams\n",
    "    ### Begin your code\n",
    "    unigram_counts = sum([len(generate_N_grams(item, 1)) for item in train_data])\n",
    "    bigram_counts = sum([len(generate_N_grams(item, 2)) for item in train_data])\n",
    "    trigram_counts = sum([len(generate_N_grams(item, 3)) for item in train_data])\n",
    "    ### End your code\n",
    "    ### return dictinaries with n-grams as keys and counts of n-grams as values\n",
    "    return unigram_counts, bigram_counts, trigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counts, bigram_counts, trigram_counts = LanguageModel(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28839 26839 24839\n"
     ]
    }
   ],
   "source": [
    "print(unigram_counts, bigram_counts, trigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (Laplace Smoothing)\n",
    "In natural language:\n",
    "*   A small number of events (e.g. words) occur with high frequency\n",
    "*   large number of events occur with very low frequency\\\n",
    "So we can’t actually evaluate our MLE models on\n",
    "unseen test data because both are likely to contain words/n-grams that these models assign zero probability to. In order to handle this problem we can use **1(laplace)smoothing** .  \n",
    "Fill out the following code cell to calculate the probability of unigram, bigram and trigram.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(d):\n",
    "    tokens = word_tokenize(d)\n",
    "    return tokens\n",
    "\n",
    "def get_tokens_freq(tkens):\n",
    "    dct = dict()\n",
    "    for item in tkens:\n",
    "        dct.update({item:tkens.count(item)})\n",
    "    return dct\n",
    "\n",
    "tokens = list()\n",
    "for i in train_data:\n",
    "    tokens.extend(get_tokens(i))\n",
    "\n",
    "for i in test_data:\n",
    "    tokens.extend(i)\n",
    "\n",
    "distinct_tokens = list(set(tokens))\n",
    "token_freq_dct = get_tokens_freq(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram_freq(n_gram):\n",
    "    dct = dict()\n",
    "    distinct_ngrams = list(set(n_gram))\n",
    "    for item in distinct_ngrams:\n",
    "        dct.update({item:n_gram.count(item)})\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = list()\n",
    "for i in train_data:\n",
    "    bigrams.extend(generate_N_grams(i, 2))\n",
    "\n",
    "bigram_freq_dct = generate_ngram_freq(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = list()\n",
    "for i in train_data:\n",
    "    trigrams.extend(generate_N_grams(i, 3))\n",
    "\n",
    "trigram_freq_dct = generate_ngram_freq(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_probability(bigram_freq, token_freq):\n",
    "    \n",
    "    probability_dct = dict()\n",
    "    \n",
    "    for bigram in list(bigram_freq.keys()):\n",
    "        items = bigram.split()\n",
    "        if len(items)>1:\n",
    "            probability_dct.update({bigram:bigram_freq[bigram]/token_freq[items[0]]})\n",
    "    return probability_dct\n",
    "\n",
    "def trigram_probability(trigram_freq, bigram_freq):\n",
    "    \n",
    "    probability_dct = dict()\n",
    "    \n",
    "    for trigram in list(trigram_freq.keys()):\n",
    "        items = trigram.split()\n",
    "        if len(items)>2:\n",
    "            bigram = items[0] + \" \" + items[1]\n",
    "            probability_dct.update({trigram:trigram_freq[trigram]/bigram_freq[bigram]})\n",
    "    return probability_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_probabilities = bigram_probability(bigram_freq_dct, token_freq_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_probabilities = trigram_probability(trigram_freq_dct, bigram_freq_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not forget to add 1(laplace)smoothing or othe methods of smoothing(which you need to explain in another markdown cell)\n",
    "def calc_unigram_p(w_1): # p(w_1)\n",
    "    ### Begin your code\n",
    "    p = token_freq_dct[w_1]/sum(list(token_freq_dct.values()))\n",
    "    ### End your code\n",
    "    return p\n",
    "    \n",
    "def calc_bigram_p(w_1,w_2): # p(w_2|w_1)\n",
    "    ### Begin your code\n",
    "    bigram = w_1 +  \" \" + w_2\n",
    "    try:\n",
    "        p =  bigram_probabilities[bigram]\n",
    "    except:\n",
    "        p = 1e-3\n",
    "    ### End your code\n",
    "    return p\n",
    "\n",
    "def calc_trigram_p(w_1,w_2,w_3): # p(w_3|w_2,w_1)\n",
    "    ### Begin your code\n",
    "    trigram = w_1 + \" \" + w_2 +\" \"+ w_3\n",
    "    try:\n",
    "        p =  trigram_probabilities[trigram]\n",
    "    except:\n",
    "        p = 1e-3\n",
    "    ### End your code\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 (Probability of sentences)\n",
    "At this part first you need to choose 2 sentence(with more than 4 words) from the test data and  calculate the probability of the chosen sentences under each language model(unigram,bigram and trigram) as expplained below:\n",
    "  \n",
    "We will treat each sentence as a sequence of terms $(w_1, \\ldots, w_n)$ whose probability under unigram model is computed as :\n",
    "$$\n",
    "P(w_1, \\ldots, w_n) = P(w_1)P(w_2)\\cdots P(w_n),\n",
    "$$\n",
    "under bigram model is computed as:\n",
    "$$\n",
    "P(w_1, \\ldots, w_n) = P(w_1)P(w_2\\mid w_1)\\cdots P(w_n\\mid w_{n-1}),\n",
    "$$\n",
    "and under trigram model is computed as:\n",
    "$$\n",
    "P(w_1, \\ldots, w_n) = P(w_1)P(w_2\\mid w_1)P(w_3\\mid w_1,w_2)\\cdots P(w_n\\mid w_{n-2},w_{n-1}),\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ### Begin your code\n",
    "    \n",
    "    def sent_prob(text):\n",
    "        p = concat_texts(text)\n",
    "        p = remove_punctuation(p)\n",
    "        print('Sentence:',p, '\\n')\n",
    "        \n",
    "        un = [calc_unigram_p(i) for i in p.split()]\n",
    "        pun = 1\n",
    "        for i in un:\n",
    "            pun *= i\n",
    "            \n",
    "        bi = generate_N_grams(p, 2)\n",
    "        pbi = calc_unigram_p(p.split()[0])\n",
    "        for i in bi:\n",
    "            j = i.split()\n",
    "            if len(j)>1:\n",
    "                pbi *= calc_bigram_p(j[0], j[1])\n",
    "        \n",
    "        tr = generate_N_grams(p, 3)\n",
    "        ptr = calc_unigram_p(p.split()[0])\n",
    "        bi = generate_N_grams(p, 2)\n",
    "        ptr += calc_bigram_p(bi[0].split()[0], bi[0].split()[1])\n",
    "        for i in tr:\n",
    "            j = i.split()\n",
    "            if len(j)>2:\n",
    "                ptr *= calc_trigram_p(j[0], j[1], j[2])\n",
    "        \n",
    "        print(f\"prob in: \\n\\tuni-gram:{pun}\\n\\tbi-gram:{pbi}\\n\\ttri-gram:{ptr}\")\n",
    "    \n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: seems difficult to find people on twitter  found nytimes  bbcscitech  but mainly hit and miss  any suggestions for finding content sources \n",
      "\n",
      "prob in: \n",
      "\tuni-gram:3.70763003254425e-71\n",
      "\tbi-gram:1.3908131701819502e-50\n",
      "\ttri-gram:1.3944084222268704e-40\n"
     ]
    }
   ],
   "source": [
    "sent_prob(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: get your brunch on before you head out to summerfest   free shuttle starting at 4pm \n",
      "\n",
      "prob in: \n",
      "\tuni-gram:4.251747805419834e-46\n",
      "\tbi-gram:1.184512760981032e-35\n",
      "\ttri-gram:4.0786446350135966e-36\n"
     ]
    }
   ],
   "source": [
    "sent_prob(test_data[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 (Finding The 3 Most Probable Next Words)\n",
    "In this part you are given a query including 2 words.  \n",
    "Query = i am  \n",
    "You need to find the 2 words that are more likely to be the Word that follows `am` under each language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_tokens(token, ngram_keys):\n",
    "    temp = list()\n",
    "    for item in ngram_keys:\n",
    "        if item.startswith(token):\n",
    "            temp.append(item)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ### Begin your code\n",
    "    import copy\n",
    "    import numpy as np\n",
    "    def most_prob(sent):\n",
    "        q = sent.split()\n",
    "        \n",
    "        # unigram model\n",
    "        dct = {}\n",
    "        for i in distinct_tokens:\n",
    "            dct.update({i:calc_unigram_p(i)})\n",
    "        key_list = list(dct.keys())\n",
    "        val_list = list(dct.values())\n",
    "        sort_list = copy.deepcopy(val_list)\n",
    "        sort_list.sort(reverse=True)\n",
    "        pos1 = val_list.index(sort_list[0])\n",
    "        pos2 = val_list.index(sort_list[1])\n",
    "        pos3 = val_list.index(sort_list[2])\n",
    "        uni = [key_list[pos1], key_list[pos2], key_list[pos3]]\n",
    "        \n",
    "        \n",
    "        # bigram model\n",
    "        bigrams = list(bigram_probabilities.keys())\n",
    "        temp = q[1]\n",
    "        tokens = list()\n",
    "        for i in range(3):\n",
    "            t = get_similar_tokens(temp, bigrams)\n",
    "            probs = [bigram_probabilities[i] for i in t]\n",
    "            max_index = np.argmax(probs)\n",
    "            temp = t[max_index].split()[-1]\n",
    "            tokens.append(temp)\n",
    "            \n",
    "        # trigram model\n",
    "        trigrams = list(trigram_probabilities.keys())\n",
    "        temp = sent\n",
    "        tr_tokens = list()\n",
    "        for i in range(3):\n",
    "            t = get_similar_tokens(temp, trigrams)\n",
    "            probs = [trigram_probabilities[i] for i in t]\n",
    "            max_index = np.argmax(probs)\n",
    "            temp = t[max_index].split()[-1]\n",
    "            tr_tokens.append(temp)\n",
    "            temp = t[max_index].split()[1] + \" \" +t[max_index].split()[2]\n",
    "        \n",
    "        \n",
    "        print(f\"Words in:\\n\\tuni-gram Model--> {uni}\\n \\tBi-gram Model--> {tokens}\\n\\tTri-gram Model--> {tr_tokens}\")\n",
    "        \n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in:\n",
      "\tuni-gram Model--> ['sb', 'sb', 'sb']\n",
      " \tBi-gram Model--> ['immediately', 'despite', 'my']\n",
      "\tBi-gram Model--> ['swag', 'then', 'their']\n"
     ]
    }
   ],
   "source": [
    "most_prob('on the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that :\n",
    "***Your answers will be evaluated based on correct implementation of the above functions.**  \n",
    "***Each task has 20 points**\n",
    "# The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3e4e885df77660ca7e0b087d607dd7176d5ad4338e5d26cc40a96e95894b80be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
